# -*- coding: utf-8 -*-
"""VKR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MdEQJkZMyQXClPOc50C-3PQHi9DR_pFj
"""

!pip install monai

import torch
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import random
import torchvision
import os
import shutil
import json
from torch.utils.data import Dataset,DataLoader
from torch import nn
import albumentations as A
from albumentations.pytorch import ToTensorV2
from pycocotools.coco import COCO
import cv2
import numpy as np
from matplotlib.patches import Polygon
from google.colab import drive,files
import zipfile
import torch.optim as optim
from tqdm import tqdm
import time
import math
from monai.data import CacheDataset
from monai.utils import set_determinism

#!pip install nnunetv2

def unzip_archive(archive_path, destination_path):
  if not os.path.exists(archive_path):
        raise FileNotFoundError(f"Архив {archive_path} не найден")
  os.makedirs(destination_path, exist_ok=True)
  try:
      with zipfile.ZipFile(archive_path, 'r') as zip_ref:
          zip_ref.extractall(destination_path)
      print(f"Архив успешно разархивирован в {destination_path}")
  except Exception as e:
      print(f"Ошибка при разархивировании: {e}")

dir_name='data'
if not os.path.exists('drive'):
  drive.mount('/content/drive')
if not os.path.exists(dir_name):
  unzip_archive('/content/drive/MyDrive/stenosis.zip',dir_name)

class MyDataset(Dataset):
  def __init__(self,data_path:str ='.',transform=None)-> None:
    self.data_path=data_path #Путь до папки датасета(в этой папке должны быть папки annotations и images, где хранятся аннотации и фотки к ним)
    if not os.path.exists(self.data_path) or not os.path.isdir(self.data_path):
      raise FileNotFoundError(f"Data directory not found: {self.data_path}")
    self.annotations_path=os.path.join(self.data_path,'annotations') # путь до папки с аннотацией
    self.images_path=os.path.join(self.data_path,'images') # путь до папки с фотографияеми
    if not os.path.exists(self.annotations_path) or not os.path.isdir(self.annotations_path):
        raise FileNotFoundError(f"Annotations directory not found: {self.annotations_path}")
    if not os.path.exists(self.images_path) or not os.path.isdir(self.images_path):
        raise FileNotFoundError(f"Images directory not found: {self.images_path}")
# Get annotations file (prefer JSON files)
    try:
        annotations_files = [f for f in os.listdir(self.annotations_path) if f.endswith('.json')]
        if not annotations_files:
            raise FileNotFoundError(f"No JSON files found in annotations directory: {self.annotations_path}")
            # Use the first JSON file
        self.annotations_file = os.path.join(self.annotations_path, annotations_files[0])
        print(f"Using annotations file: {self.annotations_file}")
    except FileNotFoundError as e:
        raise FileNotFoundError(f"Annotations directory issue: {str(e)}")
    try:
        #with open(self.annotations_file, 'r') as f:{
            #self.annotations = json.load(f)
        self.annotations = COCO(self.annotations_file)
        print(f'Number of images in {self.annotations_file} : {len(self.annotations.imgs)}')
        print(f'Number of category in {self.annotations_file} : {len(self.annotations.cats)}')
        print(f'Number of anns in {self.annotations_file} : {len(self.annotations.getAnnIds())}')
        category_ids = set()
        for i,annotation in self.annotations.anns.items():
          category_ids.add(annotation['category_id'])
        print(f'Number of classes: {len(category_ids)}, Unique classes:{category_ids} \n')
    except json.JSONDecodeError:
        raise ValueError(f"Invalid JSON format in annotations file: {self.annotations_file}")
    except FileNotFoundError:
        raise FileNotFoundError(f"Could not open annotations file: {self.annotations_file}")
    self.transform=transform


  def __len__(self):
    return len(self.annotations.getImgIds())
  def __getitem__(self, idx:int):
    img_id = self.annotations.getImgIds()[idx]
    img_info = self.annotations.loadImgs(img_id)[0]
    image_path = os.path.join(self.images_path, img_info['file_name'])
    image = cv2.imread(image_path)#, cv2.IMREAD_GRAYSCALE)
    image=np.transpose(image,(2,1,0))
    #image=np.expand_dims(image,axis=0)
    #image=image.astype(np.float32)


    anns=self.annotations.dataset['annotations']
    relevant_anns = [ann for ann in anns if ann['image_id'] == img_id]
    height, width = img_info['height'], img_info['width']
    mask = np.zeros((3,height, width), dtype=np.uint8)
    for ann in relevant_anns:
      coords=ann['segmentation'][0]
      points = np.array([(coords[i], coords[i+1]) for i in range(0, len(coords), 2)], dtype=np.int32)
      points = points.reshape((-1, 1, 2))
      cv2.fillPoly(mask[0], [points], color=1)
    # Аугментация
    augmented = self.transform(image=image, mask=mask)
    image = augmented['image']  # (C, H, W), уже в тензоре
    mask = augmented['mask'].float()  # (1, H, W)


    return image, mask
  def create_grid_(self,number_samples:int=4):
    rows = max(1, math.floor(math.sqrt(number_samples)))
    cols=math.ceil(number_samples/rows)
    while rows * cols < number_samples:
        rows += 1
        cols = math.ceil(number_samples /cols)
    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))
    # Преобразуем axes в двумерный массив
    if rows == 1 and cols == 1:
        axes = np.array([[axes]])
    elif rows == 1:
        axes = np.array([axes])
    elif cols == 1:
        axes = np.array([[ax] for ax in axes])
    else:
        axes = np.array(axes)
    return fig,axes
  def remove_null_subplots_(self,number_samples:int,axes):
    rows=axes.shape[0]
    cols=axes.shape[1]
    for row in range(rows):
      for col in range(cols):
        plot_idx = row * cols + col
        if plot_idx >= number_samples:
            axes[row][col].remove()  # Удаляем пустые подграфики
            continue

  def display_image(self,idx:int=0,axes=None):
    # Проверяем корректность индекса
    if idx < 0 or idx >= len(self.annotations.getImgIds()):
        print(f"Ошибка: индекс {idx} вне диапазона [0, {len(self.annotations.getImgIds())-1}]")
        return
    img_id = self.annotations.getImgIds()[idx]
    img_info = self.annotations.loadImgs(img_id)[0]
    image_path = os.path.join(self.images_path, img_info['file_name'])
    image = cv2.imread(image_path)
    if image is None:
        print(f"Не удалось загрузить изображение: {image_path}")
        return
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    if not axes:
      plt.imshow(image)
      plt.title(img_info['file_name'],fontsize=8)
      plt.axis('off')
    else:
      axes.imshow(image)
      axes.set_title(img_info['file_name'],fontsize=8)
      axes.axis('off')
  def display_annotation(self,idx:int=0,axes=None):
    img_id = self.annotations.getImgIds()[idx]
    img_info = self.annotations.loadImgs(img_id)[0]
    anns=self.annotations.dataset['annotations']
    relevant_anns = [ann for ann in anns if ann['image_id'] == img_id]
    for ann in relevant_anns:
      if 'bbox' in ann:
            # Формат bbox в COCO: [x, y, width, height]
          x, y, w, h = ann['bbox']
          rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')
          if not axes:
            plt.gca().add_patch(rect)
          else:
            axes.add_patch(rect)

                    # # Опционально: добавляем метку категории
          if 'category_id' in ann:
            cat_id = ann['category_id']
            cat_name = self.annotations.loadCats([cat_id])[0]['name']
            if not axes:
              plt.gca().set_title(f"file: {img_info['file_name']}. class : {cat_name}",fontsize=8)
            else:
              axes.set_title(f"file: {img_info['file_name']}. class : {cat_name}",fontsize=8)
# Полигональная сегментация (синий)
          if 'segmentation' in ann and isinstance(ann['segmentation'], list):
              for seg in ann['segmentation']:
                        # Проверяем, что сегментация в формате полигона (список координат)
                      if isinstance(seg, list) and len(seg) >= 6:  # Минимум 3 точки для полигона
                            # Преобразуем [x1, y1, x2, y2, ...] в [[x1, y1], [x2, y2], ...]
                          poly_coords = np.array(seg).reshape(-1, 2)
                          polygon = Polygon(poly_coords, closed=True, linewidth=2, edgecolor='b', facecolor='none')
                          if not axes:
                            plt.gca().add_patch(polygon)
                          else:
                            axes.add_patch(polygon)
                        # Пропускаем RLE или некорректные форматы
                      else:
                          print(f"Пропущена некорректная сегментация для аннотации {ann['id']}")
  def display_image_with_annotation(self,idx:int=0,axes=None):
      self.display_image(idx,axes)
      self.display_annotation(idx,axes)
  def display_random_images(self,number_samples=4):
    if number_samples>self.__len__():
      raise ValueError(f'number_sample > {self.__len__()}')
      return
    fig, axes = self.create_grid_(number_samples)
    rows=axes.shape[0]
    cols=axes.shape[1]
    for row in range(rows):
      for col in range(cols):
        idx=random.randint(0,len(self.annotations.getImgIds()))
        self.display_image(idx,axes[row][col])
    self.remove_null_subplots_(number_samples,axes)
    plt.tight_layout()
    plt.show()

  def display_random_images_with_annotations(self,number_samples:int=4):
    if number_samples>self.__len__():
      raise ValueError(f'number_sample > {self.__len__()}')
      return
    fig, axes = self.create_grid_(number_samples)
    rows=axes.shape[0]
    cols=axes.shape[1]
    for row in range(rows):
      for col in range(cols):
        idx=random.randint(0,len(self.annotations.getImgIds()))
        self.display_image_with_annotation(idx,axes[row][col])
    self.remove_null_subplots_(number_samples,axes)
    plt.tight_layout()
    plt.show()
  def display_random_images_for_compare(self,number_samples:int=4):
    if number_samples>self.__len__():
      raise ValueError(f'number_sample > {self.__len__()}')
      return
    fig, axes1 = self.create_grid_(number_samples)
    fig, axes2 = self.create_grid_(number_samples)
    rows=axes1.shape[0]
    cols=axes1.shape[1]
    total_subplots = rows * cols
    indexes = random.sample(range(self.__len__()), total_subplots)
    idx_counter=0

    for row in range(rows):
      for col in range(cols):
        if idx_counter < len(indexes):
              self.display_image(indexes[idx_counter], axes1[row][col])
              idx_counter += 1
    self.remove_null_subplots_(number_samples,axes1)
    idx_counter=0
    for row in range(rows):
        for col in range(cols):
            if idx_counter < len(indexes):
                self.display_image_with_annotation(indexes[idx_counter], axes2[row][col])
                idx_counter += 1
    self.remove_null_subplots_(number_samples,axes2)
    plt.tight_layout()
    plt.show()
  def display_random_images_for_compare1(self, number_samples: int = 4):
    if number_samples > self.__len__():
        raise ValueError(f'number_samples > {self.__len__()}')

    # Create a single figure with a 1x2 layout
    fig, axes = plt.subplots(1, 2, figsize=(10, 10))  # Adjust figsize as needed

    # Determine grid size for subplots
    rows = int(number_samples ** 0.5) + (1 if number_samples % int(number_samples ** 0.5) else 0)
    cols = (number_samples + rows - 1) // rows
    total_subplots = rows * cols

    # Generate random indices for images
    indexes = random.sample(range(self.__len__()), total_subplots)
    print(indexes)  # For debugging

    # Get the position of the left and right main Axes
    pos1 = axes[0].get_position()  # Bbox for left Axes
    pos2 = axes[1].get_position()  # Bbox for right Axes

    # Remove the original Axes to replace with subgrids
    axes[0].remove()
    axes[1].remove()

    # Create subgrid for left side (axes1)
    gs1 = fig.add_gridspec(rows, cols, figure=fig,
                           left=pos1.x0, right=pos1.x1,
                           top=pos1.y1, bottom=pos1.y0)
    axes1 = np.array([[fig.add_subplot(gs1[r, c]) for c in range(cols)] for r in range(rows)])

    # Create subgrid for right side (axes2)
    gs2 = fig.add_gridspec(rows, cols, figure=fig,
                           left=pos2.x0, right=pos2.x1,
                           top=pos2.y1, bottom=pos2.y0)
    axes2 = np.array([[fig.add_subplot(gs2[r, c]) for c in range(cols)] for r in range(rows)])

    # Populate left grid (axes1) with images
    idx_counter = 0
    for row in range(rows):
        for col in range(cols):
            if idx_counter < len(indexes):
                self.display_image(indexes[idx_counter], axes1[row][col])
                idx_counter += 1
    self.remove_null_subplots_(number_samples, axes1)

    # Populate right grid (axes2) with annotated images
    idx_counter = 0
    for row in range(rows):
        for col in range(cols):
            if idx_counter < len(indexes):
                self.display_image_with_annotation(indexes[idx_counter], axes2[row][col])
                idx_counter += 1
    self.remove_null_subplots_(number_samples, axes2)

    # Adjust layout and display
    fig.tight_layout()
    plt.show()

ds_train=MyDataset(data_path='data/stenosis/train', transform=strong_transforms)
plt.imshow(ds_train[0][0])



#Архитектура модели (ResNeXtUNetCoronary2D)
class ConvRelu(nn.Module):
    def __init__(self, in_channels, out_channels, kernel, padding, dropout=0.0):
        super().__init__()
        self.convrelu = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel, padding=padding, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()
        )

    def forward(self, x):
        return self.convrelu(x)

class AttentionGate(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.Wg = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        self.Wx = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        self.psi = nn.Sequential(
            nn.Conv2d(out_channels, 1, kernel_size=1, bias=False),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        g1 = self.Wg(g)
        x1 = self.Wx(x)
        psi = self.relu(g1 + x1)
        psi = self.psi(psi)
        return x * psi

class ASPP(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = ConvRelu(in_channels, out_channels, 1, 0)
        self.atrous1 = ConvRelu(in_channels, out_channels, 3, 2, dropout=0.1)
        self.atrous2 = ConvRelu(in_channels, out_channels, 3, 4, dropout=0.1)
        self.atrous3 = ConvRelu(in_channels, out_channels, 3, 8, dropout=0.1)
        self.global_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            ConvRelu(in_channels, out_channels, 1, 0)
        )
        self.final_conv = ConvRelu(out_channels * 5, out_channels, 1, 0)

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.atrous1(x)
        x3 = self.atrous2(x)
        x4 = self.atrous3(x)
        x5 = self.global_pool(x)
        x5 = nn.functional.interpolate(x5, size=x.size()[2:], mode='bilinear', align_corners=True)
        x = torch.cat([x1, x2, x3, x4, x5], dim=1)
        return self.final_conv(x)

class DecoderBlock(nn.Module):
    def __init__(self, in_channels, out_channels, dropout=0.1):
        super().__init__()
        self.conv1 = ConvRelu(in_channels, in_channels // 4, 1, 0, dropout)
        self.deconv = nn.ConvTranspose2d(
            in_channels // 4, in_channels // 4, kernel_size=4, stride=2, padding=1, output_padding=0
        )
        self.conv2 = ConvRelu(in_channels // 4, out_channels, 1, 0, dropout)
        self.attention = AttentionGate(out_channels, out_channels)

    def forward(self, x, skip):
        x = self.conv1(x)
        x = self.deconv(x)
        x = self.conv2(x)
        skip = self.attention(x, skip)
        return x + skip

class ResNeXtUNetCoronary2D(nn.Module):
    def __init__(self, n_classes=1, base_filters=32, deep_supervision=True):
        super().__init__()
        self.n_classes = n_classes
        self.deep_supervision = deep_supervision

        self.base_model = resnext50_32x4d(weights="DEFAULT")
        self.base_layers = list(self.base_model.children())
        filters = [256, 512, 1024, 2048]

        self.encoder0 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        self.encoder1 = self.base_layers[4]
        self.encoder2 = self.base_layers[5]
        self.encoder3 = self.base_layers[6]
        self.encoder4 = self.base_layers[7]

        self.bottleneck = nn.Sequential(
            ASPP(filters[3], filters[3] // 2),
            ConvRelu(filters[3] // 2, filters[3], 3, 1, dropout=0.2)
        )

        self.decoder4 = DecoderBlock(filters[3], filters[2], dropout=0.2)
        self.decoder3 = DecoderBlock(filters[2], filters[1], dropout=0.15)
        self.decoder2 = DecoderBlock(filters[1], filters[0], dropout=0.1)
        self.decoder1 = DecoderBlock(filters[0], base_filters, dropout=0.1)

        self.final_conv = nn.Sequential(
            ConvRelu(base_filters, base_filters // 2, 3, 1, dropout=0.1),
            nn.Conv2d(base_filters // 2, n_classes, 3, padding=1)
        )

        if self.deep_supervision:
            self.supervision4 = nn.Conv2d(filters[2], n_classes, 1)
            self.supervision3 = nn.Conv2d(filters[1], n_classes, 1)
            self.supervision2 = nn.Conv2d(filters[0], n_classes, 1)
            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

    def forward(self, x):
        e0 = self.encoder0(x)
        e1 = self.encoder1(e0)
        e2 = self.encoder2(e1)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)

        b = self.bottleneck(e4)

        d4 = self.decoder4(b, e3)
        d3 = self.decoder3(d4, e2)
        d2 = self.decoder2(d3, e1)
        d1 = self.decoder1(d2)

        out = self.final_conv(d1)
        out = torch.sigmoid(out)

        if self.deep_supervision and self.training:
            s4 = self.supervision4(d4)
            s4 = self.upsample(self.upsample(self.upsample(s4)))
            s3 = self.supervision3(d3)
            s3 = self.upsample(self.upsample(s3))
            s2 = self.supervision2(d2)
            s2 = self.upsample(s2)
            return [out, s4, s3, s2]
        return out
    def get_loss(self, outputs, target, criterion=None):
        if criterion is None:
            raise ValueError("Loss function must be provided")

        if self.deep_supervision and self.training:
            loss = 0
            weights = [1.0, 0.4, 0.3, 0.2]
            for i, output in enumerate(outputs):
                loss += weights[i] * criterion(output, target)
            return loss
        return criterion(outputs, target)

# Image size parameters
IMG_SIZE = 512  # Original image size (for reference)
PATCH_SIZE = 256  # Target patch size for cropping/resizing
strong_transforms = A.Compose([
    A.RandomResizedCrop(size=(PATCH_SIZE,PATCH_SIZE), scale=(0.08, 1.0), p=1.0),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.Transpose(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.RandomGamma(gamma_limit=(80, 120), p=0.25),
    A.Emboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=0.25),
    A.Blur(blur_limit=3, p=0.01),
    A.OneOf([
        A.ElasticTransform(alpha=120, sigma=120 * 0.05, p=0.5),
        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),
        A.OpticalDistortion(distort_limit=1.0, p=1.0),
    ], p=0.8),
    A.Normalize(mean=0.0, std=1.0),  # Single-channel normalization
    ToTensorV2(),
], additional_targets={'mask': 'mask'})

# Validation augmentation pipeline
transforms = A.Compose([
    A.Resize(PATCH_SIZE,PATCH_SIZE, p=1.0),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.Transpose(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),
    A.Normalize(mean=0.0, std=1.0),  # Single-channel normalization
    ToTensorV2(),
], additional_targets={'mask': 'mask'})

def double_conv(in_channels,out_channels):
  return nn.Sequential(
      nn.Conv2d(in_channels,out_channels,3,padding=1),
      nn.ReLU(inplace=True),
      nn.Conv2d(out_channels,out_channels,3,padding=1),
      nn.ReLU(inplace=True)
  )
class UNet(nn.Module):
  def __init__(self,n_classes):
    super().__init__()
    self.conv_down1=double_conv(3,64)
    self.conv_down2=double_conv(64,128)
    self.conv_down3=double_conv(128,256)
    self.conv_down4=double_conv(256,512)

    self.maxpool=nn.MaxPool2d(2)
    self.upsample=nn.Upsample(scale_factor=2,mode='bilinear',align_corners=True)

    self.conv_up3 = double_conv(256 + 512, 256)
    self.conv_up2 = double_conv(128 + 256, 128)
    self.conv_up1 = double_conv(128 + 64, 64)

    self.last_conv = nn.Conv2d(64, n_classes, kernel_size=1)
  def forward(self,x):
    # Batch - 1d tensor.  N_channels - 1d tensor, IMG_SIZE - 2d tensor.
    # Example: x.shape >>> (10, 3, 256, 256).
    conv1 = self.conv_down1(x)  # <- BATCH, 3, IMG_SIZE  -> BATCH, 64, IMG_SIZE..
    x = self.maxpool(conv1)     # <- BATCH, 64, IMG_SIZE -> BATCH, 64, IMG_SIZE 2x down.
    conv2 = self.conv_down2(x)  # <- BATCH, 64, IMG_SIZE -> BATCH,128, IMG_SIZE.
    x = self.maxpool(conv2)     # <- BATCH, 128, IMG_SIZE -> BATCH, 128, IMG_SIZE 2x down.
    conv3 = self.conv_down3(x)  # <- BATCH, 128, IMG_SIZE -> BATCH, 256, IMG_SIZE.
    x = self.maxpool(conv3)     # <- BATCH, 256, IMG_SIZE -> BATCH, 256, IMG_SIZE 2x down.
    x = self.conv_down4(x)      # <- BATCH, 256, IMG_SIZE -> BATCH, 512, IMG_SIZE.
    x = self.upsample(x) # <- BATCH, 512, IMG_SIZE -> BATCH, 512, IMG_SIZE 2x up.

    #(Below the same)                                 N this       ==        N this.  Because the first N is upsampled.
    x = torch.cat([x, conv3], dim=1) # <- BATCH, 512, IMG_SIZE & BATCH, 256, IMG_SIZE--> BATCH, 768, IMG_SIZE.

    x = self.conv_up3(x) #  <- BATCH, 768, IMG_SIZE --> BATCH, 256, IMG_SIZE.
    x = self.upsample(x)  #  <- BATCH, 256, IMG_SIZE -> BATCH,  256, IMG_SIZE 2x up.
    x = torch.cat([x, conv2], dim=1) # <- BATCH, 256,IMG_SIZE & BATCH, 128, IMG_SIZE --> BATCH, 384, IMG_SIZE.

    x = self.conv_up2(x) # <- BATCH, 384, IMG_SIZE --> BATCH, 128 IMG_SIZE.
    x = self.upsample(x)   # <- BATCH, 128, IMG_SIZE --> BATCH, 128, IMG_SIZE 2x up.
    x = torch.cat([x, conv1], dim=1) # <- BATCH, 128, IMG_SIZE & BATCH, 64, IMG_SIZE --> BATCH, 192, IMG_SIZE.

    x = self.conv_up1(x) # <- BATCH, 128, IMG_SIZE --> BATCH, 64, IMG_SIZE.

    out = self.last_conv(x) # <- BATCH, 64, IMG_SIZE --> BATCH, n_classes, IMG_SIZE.
    out = torch.sigmoid(out)

    return out

class ConvReluUpsample(nn.Module):
    def __init__(self, in_channels, out_channels, upsample=False):
        super().__init__()
        self.upsample = upsample
        self.make_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        self.block = nn.Sequential(
            nn.Conv2d(
                in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False
            ),
            nn.GroupNorm(32, out_channels),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        x = self.block(x)
        if self.upsample:
            x = self.make_upsample(x)
        return x
class SegmentationBlock(nn.Module):
    def __init__(self, in_channels, out_channels, n_upsamples=0):
        super().__init__()

        blocks = [ConvReluUpsample(in_channels, out_channels, upsample=bool(n_upsamples))]

        if n_upsamples > 1:
            for _ in range(1, n_upsamples):
                blocks.append(ConvReluUpsample(out_channels, out_channels, upsample=True))

        self.block = nn.Sequential(*blocks)

    def forward(self, x):
        return self.block(x)
class FPN(nn.Module):

    def __init__(self, n_classes=1,
                 pyramid_channels=256,
                 segmentation_channels=256):
        super().__init__()

        # Bottom-up layers
        self.conv_down1 = double_conv(3, 64)
        self.conv_down2 = double_conv(64, 128)
        self.conv_down3 = double_conv(128, 256)
        self.conv_down4 = double_conv(256, 512)
        self.conv_down5 = double_conv(512, 1024)
        self.maxpool = nn.MaxPool2d(2)

        # Top layer
        self.toplayer = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels
        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)

        # Lateral layers
        self.latlayer1 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)
        self.latlayer2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)
        self.latlayer3 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)

        # Segmentation block layers
        self.seg_blocks = nn.ModuleList([
            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)
            for n_upsamples in [0, 1, 2, 3]
        ])
        # Last layer
        self.last_conv = nn.Conv2d(256, n_classes, kernel_size=1, stride=1, padding=0)

    def upsample_add(self, x, y):
        _,_,H,W = y.size()
        upsample = nn.Upsample(size=(H,W), mode='bilinear', align_corners=True)

        return upsample(x) + y

    def upsample(self, x, h, w):
        sample = nn.Upsample(size=(h, w), mode='bilinear', align_corners=True)
        return sample(x)
    def forward(self, x):

        # Bottom-up
        c1 = self.maxpool(self.conv_down1(x))
        c2 = self.maxpool(self.conv_down2(c1))
        c3 = self.maxpool(self.conv_down3(c2))
        c4 = self.maxpool(self.conv_down4(c3))
        c5 = self.maxpool(self.conv_down5(c4))

        # Top-down
        p5 = self.toplayer(c5)
        p4 = self.upsample_add(p5, self.latlayer1(c4))
        p3 = self.upsample_add(p4, self.latlayer2(c3))
        p2 = self.upsample_add(p3, self.latlayer3(c2))

        # Smooth
        p4 = self.smooth1(p4)
        p3 = self.smooth2(p3)
        p2 = self.smooth3(p2)

          # Segmentation
        _, _, h, w = p2.size()
        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p2, p3, p4, p5])]

        out = self.upsample(self.last_conv(sum(feature_pyramid)), 4 * h, 4 * w)

        out = torch.sigmoid(out)
        return out

from torchvision.models import resnext50_32x4d

class ConvRelu(nn.Module):
    def __init__(self, in_channels, out_channels, kernel, padding):
        super().__init__()

        self.convrelu = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel, padding=padding),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.convrelu(x)
        return x
class DecoderBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = ConvRelu(in_channels, in_channels // 4, 1, 0)

        self.deconv = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4,
                                          stride=2, padding=1, output_padding=0)

        self.conv2 = ConvRelu(in_channels // 4, out_channels, 1, 0)

    def forward(self, x):
        x = self.conv1(x)
        x = self.deconv(x)
        x = self.conv2(x)

        return x
class ResNeXtUNet(nn.Module):

    def __init__(self, n_classes):
        super().__init__()

        self.base_model = resnext50_32x4d(pretrained=True)
        self.base_layers = list(self.base_model.children())
        filters = [4*64, 4*128, 4*256, 4*512]

        # Down
        self.encoder0 = nn.Sequential(*self.base_layers[:3])
        self.encoder1 = nn.Sequential(*self.base_layers[4])
        self.encoder2 = nn.Sequential(*self.base_layers[5])
        self.encoder3 = nn.Sequential(*self.base_layers[6])
        self.encoder4 = nn.Sequential(*self.base_layers[7])

        # Up
        self.decoder4 = DecoderBlock(filters[3], filters[2])
        self.decoder3 = DecoderBlock(filters[2], filters[1])
        self.decoder2 = DecoderBlock(filters[1], filters[0])
        self.decoder1 = DecoderBlock(filters[0], filters[0])

        # Final Classifier
        self.last_conv0 = ConvRelu(256, 128, 3, 1)
        self.last_conv1 = nn.Conv2d(128, n_classes, 3, padding=1)
    def forward(self, x):
        # Down
        x = self.encoder0(x)
        e1 = self.encoder1(x)
        e2 = self.encoder2(e1)
        e3 = self.encoder3(e2)
        e4 = self.encoder4(e3)

        # Up + sc
        d4 = self.decoder4(e4) + e3
        d3 = self.decoder3(d4) + e2
        d2 = self.decoder2(d3) + e1
        d1 = self.decoder1(d2)
        #print(d1.shape)
  # final classifier
        out = self.last_conv0(d1)
        out = self.last_conv1(out)
        out = torch.sigmoid(out)

        return out
    def get_loss(self, outputs, target, criterion=None):
        if criterion is None:
            raise ValueError("Loss function must be provided")

        if self.deep_supervision and self.training:
            loss = 0
            weights = [1.0, 0.4, 0.3, 0.2]
            for i, output in enumerate(outputs):
                loss += weights[i] * criterion(output, target)
            return loss
        return criterion(outputs, target)

def dice_coef_metric(inputs, target):
    intersection = 2.0 * (target * inputs).sum()
    union = target.sum() + inputs.sum()
    if target.sum() == 0 and inputs.sum() == 0:
        return 1.0

    return intersection / union
def dice_coef_loss(inputs, target):
    smooth = 1.0
    intersection = 2.0 * ((target * inputs).sum()) + smooth
    union = target.sum() + inputs.sum() + smooth

    return 1 - (intersection / union)
def bce_dice_loss(inputs, target):
    dicescore = dice_coef_loss(inputs, target)
    bcescore = nn.BCELoss()
    bceloss = bcescore(inputs, target)

    return bceloss + dicescore

def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs):
  print(model_name)
  loss_history = []
  train_history = []
  val_history = []
  best_dice=0.0

  for epoch in range(num_epochs):
      model.train() # Enter train mode

      losses = []
      train_iou = []

      if lr_scheduler:

          warmup_factor = 1.0 / 100
          warmup_iters = min(100, len(train_loader) - 1)
          lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)
      for i_step, (data, target) in enumerate(train_loader):
            data = data.to(device)
            target = target.to(device)

            outputs = model(data)

            out_cut = np.copy(outputs.data.cpu().numpy())
            out_cut[np.nonzero(out_cut < 0.5)] = 0.0
            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0

            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())

            loss = train_loss(outputs, target)

            losses.append(loss.item())
            train_iou.append(train_dice)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if lr_scheduler:
                lr_scheduler.step()
      val_mean_iou = compute_iou(model, val_loader)

      loss_history.append(np.array(losses).mean())
      train_history.append(np.array(train_iou).mean())
      val_history.append(val_mean_iou)

      if val_mean_iou > best_dice:
            best_dice = val_mean_iou
            torch.save(model.state_dict(), f"best_model.pth")
            print(f"Best model saved at epoch {epoch} with Dice: {val_mean_iou:.4f}")

      print("Epoch [%d]" % (epoch))
      print("Mean loss on train:", np.array(losses).mean(),
              "\nMean DICE on train:", np.array(train_iou).mean(),
              "\nMean DICE on validation:", val_mean_iou)

  return loss_history, train_history, val_history
def compute_iou(model, loader, threshold=0.3):
    """
    Computes accuracy on the dataset wrapped in a loader

    Returns: accuracy as a float value between 0 and 1
    """
    #model.eval()
    valloss = 0

    with torch.no_grad():

        for i_step, (data, target) in enumerate(loader):

            data = data.to(device)
            target = target.to(device)
            #prediction = model(x_gpu)

            outputs = model(data)
           # print("val_output:", outputs.shape)

            out_cut = np.copy(outputs.data.cpu().numpy())
            out_cut[np.nonzero(out_cut < threshold)] = 0.0
            out_cut[np.nonzero(out_cut >= threshold)] = 1.0

            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())
            valloss += picloss

        #print("Threshold:  " + str(threshold) + "  Validation DICE score:", valloss / i_step)

    return valloss / i_step
# lr_scheduler
def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):
    def f(x):
        if x >= warmup_iters:
            return 1
        alpha = float(x) / warmup_iters
        return warmup_factor * (1 - alpha) + alpha

    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)

def show_aug(inputs, nrows=5, ncols=5, image=True):
    plt.figure(figsize=(10, 10))
    plt.subplots_adjust(wspace=0., hspace=0.)
    i_ = 0

    if len(inputs) > 25:
        inputs = inputs[:25]

    for idx in range(len(inputs)):

        # normalization
        if image is True:
            img = inputs[idx].numpy().transpose(1,2,0)
            mean = [0.485, 0.456, 0.406]
            std = [0.229, 0.224, 0.225]
            img = (img*std+mean).astype(np.float32)
        else:
            img = inputs[idx].numpy().astype(np.float32)
            img = img[0,:,:]

        #plot
        #print(img.max(), len(np.unique(img)))
        plt.subplot(nrows, ncols, i_+1)
        plt.imshow(img);
        plt.axis('off')

        i_ += 1

    return plt.show()

device='cuda' if torch.cuda.is_available() else 'cpu'

#unet = UNet(n_classes=1).to(device)

#fpn = FPN().to(device)

rx50 = ResNeXtUNet(n_classes=1).to(device)
#rx_next=ResNeXtUNetCoronary2D(n_classes=1,base_filters=16,deep_supervision=True).to(device)

# Optimizers
#unet_optimizer = torch.optim.Adamax(unet.parameters(), lr=1e-3)
#fpn_optimizer = torch.optim.Adamax(fpn.parameters(), lr=1e-3)
rx50_optimizer = torch.optim.Adam(rx50.parameters(), lr=5e-4)
#rx_next_optimizer=torch.optim.AdamW(rx_next.parameters(), lr=5e-4)

ds_train=MyDataset(data_path='data/stenosis/train',transform=strong_transforms)
ds_val=MyDataset(data_path='data/stenosis/val',transform=transforms)
#ds_test=MyDataset('data/stenosis/test',transform=,mode='viz')

dl_train=DataLoader(ds_train,batch_size=4,shuffle=True)
dl_val=DataLoader(ds_val,batch_size=4,shuffle=False)
#dl_test=DataLoader(ds_test,batch_size=8,shuffle=False)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# num_ep = 61
# # Train UNet
# #unet_lh, unet_th, unet_vh = train_model("Vanila_UNet", unet, dl_train, dl_val, bce_dice_loss, unet_optimizer, False, num_ep)
# 
# # Train FPN
# 
# #fpn_lh, fpn_th, fpn_vh = train_model("FPN", fpn, dl_train, dl_val, bce_dice_loss, fpn_optimizer, False, num_ep)#
# 
# # Train ResNeXt50
# rx50_lh, rx50_th, rx50_vh = train_model("ResNeXt50", rx50, dl_train, dl_val, bce_dice_loss, rx50_optimizer, False, num_ep)
# #rx_next_lh, rx_next_th, rx_next_vh = train_model("ResNeXt_new50", rx_next, dl_train, dl_val, bce_dice_loss, rx_next_optimizer, False, num_ep)

ds_train[0][1].unique()

rx50.load_state_dict(torch.load('best_model_epoch_60.pth'))

def plot_model_history(model_name,
                        train_history, val_history,
                        num_epochs):

    x = np.arange(num_epochs)

    fig = plt.figure(figsize=(10, 6))
    plt.plot(x, train_history, label='train dice', lw=3, c="springgreen")
    plt.plot(x, val_history, label='validation dice', lw=3, c="deeppink")

    plt.title(f"{model_name}", fontsize=15)
    plt.legend(fontsize=12)
    plt.xlabel("Epoch", fontsize=15)
    plt.ylabel("DICE", fontsize=15)

    fn = str(int(time.time())) + ".png"
    plt.show()

#plot_model_history("UNet with ResNeXt50 backbone", rx50_th, rx50_vh, num_ep)

test_iou = compute_iou(rx50, dl_test)
print(f"""ResNext50\nMean IoU of the test images - {np.around(test_iou, 2)*100}%""")

rx50_new = ResNeXtUNetCoronary2D(n_classes=1, base_filters=32, deep_supervision=True).to(device)
#rx50_new_criterion = BCE_Dice_Loss(bce_weight=0.5, class_weight=10.0)
rx50_new_optimizer = torch.optim.AdamW(rx50_new.parameters(), lr=1e-3)

num_ep=61
# Train ResNeXt50
rx50_lh, rx50_th, rx50_vh = train_model("ResNeXt50_New", rx50_new, dl_train, dl_val, bce_dice_loss, rx50_new_optimizer, True, num_ep)